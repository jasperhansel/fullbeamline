\chapter{Optimization: Lattice Correction and Design}
\label{c:opti}

\index{optimization|hyperbf}
\index{merit function}
This chapter covers the process of \vn{optimization} which involves minimization of a
\vn{Merit Function}. Optimization can be used to correct or to design lattices. Examples of \vn{lattice
corrections} include flattening the orbit and adjusting quadrupoles to correct the
measured betatron phase. \vn{Lattice design} involves creating a lattice that conforms to
a set of desirable properties. For example, requiring that the beta function in a certain
region never exceeds a given value. In this chapter,
Section~\sref{s:lattice.correction} presents the merit function in the context of lattice
corrections while Section~\sref{s:lattice.design} discuses the merit function in the
context of lattice design. Since the concepts used in \vn{lattice corrections} and
\vn{lattice design} are similar, \tao combines the two into one generalized process as
discussed in Section~\sref{s:generalized.design}.

%------------------------------------------------------------------------
\section{Lattice Corrections}
\label{s:lattice.correction}
\index{modeling data}
\index{lattice corrections}

\index{optimization!merit function}
Consider the problem of problem of modifying the orbit of a beam through a lattice to
conform to some desired orbit (typically a ``flat'' orbit running through the centers of
the quadrupoles). The process generally goes through three stages: First the orbit is
measured, then corrections to the steering elements are calculated and finally the
corrections are applied to the machine. Since these are necessarily machine specific, \tao
has no specific routines to measure orbits or to load steering corrections but they could
be implemented with some custom coding as discussed in Chapter~\sref{c:custom.tao}. What
\tao does, however, is to implement a generalized algorithm procedure for minimizing a
\vn{merit function} which can be used to calculate the corrections.  The idea is to vary a
set of variables (steerings in the case of an orbit correction) within the \vn{model}
lattice (\sref{s:universe}) with the aim to make the \vn{measured} \vn{data} (position
data for an orbit correction) correspond to the values as calculated from the \vn{model}
lattice.  Once the model lattice the \vn{model} and \vn{measured} data agree, the
difference between the \vn{model}, which represents the state of the machine when the
measurement is made, and the \vn{design}, which represents the desired state of the
machine, is used to calculate corrections. In the case of flattening an orbit, the
difference between the \vn{model} steering strengths and the \vn{design} steering
strengths (typically the \vn{design} steering strengths are zero) is what the real
steerings need to be changed by to flatten the orbit.

The merit function \vn{M} that is a measure of how well the data as calculated from the
\vn{model}, fits the measured data. \tao uses a merit function of the form
\Begineq
  {\cal M} \equiv 
    \sum_{i} w_i \, \bigl[ \delta D_i \bigr]^2 + 
    \sum_{j} w_j \, \bigl[ \delta V_j \bigr]^2
  \label{m1}
\Endeq
where
\begin{align}
  \delta D &= \data\_\model - \data\_\meas \CRNO
  \delta V &= \var\_\model  - \var\_\meas
  \label{dd1}
\end{align}
\vn{data_model} is the data as calculated from the \vn{model} and \vn{data_meas} is the
measured data. \vn{var_model} is the value of a variable in the \vn{model} and
\vn{var_meas} is the value as measured at the time the data was taken (for example, by
measuring the current through a steering and using a calibration factor to calculate the
kick) and the sum \vn{j} runs over all variables that are allowed to be varied to minimize
\vn{M}. The second term in the merit function prevents degeneracies (or near
degeneracies) in the problem which would allow \tao to find solutions where
\vn{data_model} matches \vn{data_measured} with the \vn{var_model} having ``unphysical''
values (values far from \vn{var_meas}). The weights $w_i$ and $w_j$ need to be set
depending upon how accurate the measured data is relative to how accurate the calibrations
for measuring the \vn{var_meas} values are. With the second term in the merit function,
the number of constraints (number of terms in the merit function) is always larger than
the number of variables and degeneracies can never occur.

In a correction one wants to change the machine variables so that the
measured data corresponds to the design values \vn{data_design}. Thus
the change in the data that one wants is
\begin{example}
  data_change = data_design - data_meas
\end{example}
Once a fit has been made, and presuming that the \vn{data_model} is
reasonably close to the \vn{data_meas} this data change within the
\vn{model} lattice can be accomplished by changing the variables by
\begin{example}
  var_change = var_design - var_model
\end{example}
This assumes the system is linear. For many situations this is true
since typically \vn{var_change} is ``small''. Since the variables have
a measured value of \vn{var_meas} the value that the variables should
be set to is
\begin{example}
  var_final = var_meas + (var_design - var_model)
\end{example}
Notice that the fitting process is independent of the \vn{design}
lattice. It is only when calculating the corrections to the
variables that the \vn{design} lattice plays a role. 

Sometimes it is desired to fit to changes in data as opposed to the absolute value of the
data. For example, when closing an orbit bump knob what is important is the difference in
orbits before and after the bump knob is varied. Designating one of these orbit the
\vn{reference}, the appropriate deltas to be used in \Eq{m1} are
\begin{align}
  \delta D &= (\data\_\model - \data\_\design) - (\data\_\meas - \data\_\reference) \CRNO
  \delta V &= (\var\_\model - \var\_\design)   - (\var\_\meas - \var\_\reference)
  \label{dd2}
\end{align}
where \vn{data_ref} and \vn{var_ref} refer to the reference measurement.  These deltas
are acceptable if the reference data is taken with the machine reasonably near the
design setup so that nonlinearities can be ignored. If this is not the case then the
fitting becomes a two step process: The first step is to fit the \vn{model} to the
\vn{reference} data using the deltas of \Eq{dd1}. The \vn{base} lattice is then set
equal to the \vn{model} lattice. The second step is to fit the model using the deltas
\begin{align}
  \delta D &= (\data\_\model - \data\_\base) - (\data\_\meas - \data\_\reference) \CRNO
  \delta V &= (\var\_\model - \var\_\base)   - (\var\_\meas - \var\_\reference)
  \label{dd3}
\end{align}

Control of what data and what variables are to be used in the fitting
process is controlled by the \vn{use}, \vn{veto}, \vn{restore}, and
\vn{clip} commands.

%------------------------------------------------------------------------
\section{Lattice Design}
\label{s:lattice.design}
\index{optimization!lattice design}
\index{optimization!constraints}

Lattice design is the process of calculating \vn{variable} strengths
to meet a number of criteria called constraints. For example, one
constraint could be that the beta function in some part of the lattice
not exceed a certain value. In this case we can proceed as was done
for lattice corrections and use \Eq{m1}. In this case, the deltas
are computed to limit values to some range so a typical delta
would be of the form
\Begineq
  \delta D \; \text{or} \; \delta V = 
    \begin{cases}
    \mbox{model} - \mbox{limit}  & \mbox{model $>$ Limit} \\
    0                            & \mbox{otherwise}
    \end{cases}
\Endeq
or a constraint is used to keep the \vn{model} at a certain value so
the form of the constraint would be
\Begineq
    \delta D \; \text{or} \; \delta V = \mbox{model} - \mbox{target}  
\Endeq
Here \vn{model} is the value as calculated from the \vn{model}
lattice. \vn{target} and \vn{limit} are given numbers. Part of the
optimization process is in deciding what the values should be for any
\vn{target} or \vn{limit}.

%------------------------------------------------------------------------
\section{Generalized Design}
\label{s:generalized.design}
\index{optimization!generalized merit function}

The form of the deltas used in the merit function is determined by two global logicals
called \vn{opt_with_ref} and \vn{opt_with_base} (\sref{s:globals}) as shown in
Table~\ref{t:delta}.
\begin{table}[ht] 
\centering 
{\tt
\begin{tabular}{lll} \toprule
  \vn{Opt_with_ref} & \vn{Opt_with_base} & \vn{delta} \\ \midrule
  F & F & model - meas                \\
  T & F & model - meas + ref - design \\
  F & T & model - meas - base         \\
  T & T & model - meas + ref - base   \\
\bottomrule
\end{tabular}
} 
\caption{The form of \vn{delta}}  
\label{t:delta}
\end{table}
An exception occurs when using a \vn{common base lattice}
(\sref{s:cbl}). In this case, the common universe does not have base
or reference values associated with it. Thus all data and variables
that are associated with the common universe calculate their
\vn{delta} as if both \vn{opt_with_ref} and \vn{opt_with_base} were
set to \vn{False}.

Another exception occurs with data when the datum value cannot be
computed (\sref{s:datum.opt}). In this case, the datum's \vn{invalid} value
is used for the \vn{delta}. This is useful, for example, in a linear
lattice when the particle trajectory results in the particle being lost.

The \vn{Non-Zero-Condition} needed for a non--zero $D_i$ is dependent
upon the \vn{merit_type} of the datum (\sref{s:data.anatomy}).
There are five \vn{merit_type} constraint
types as given in Table~\ref{t:con.type}.
\begin{table}[ht]
\centering
{\tt
\begin{tabular}{|l|l|l|} \toprule
  {\it Merit\_Type}       & {\it Non-zero-Condition} \\ \midrule
  \vn{target}            & Any \vn{delta}   \\
  \vn{min}, \vn{abs_min} & \vn{delta} $<$ 0 \\
  \vn{max}, \vn{abs_max} & \vn{delta} $>$ 0 \\
\bottomrule
\end{tabular}
}
\caption{Constraint Type List.}
\label{t:con.type}
\end{table}
\index{optimization!optimize with reference}

For variables, the form of the terms $V_i$ is determined by its \vn{merit_type}.
Here the \vn{merit_type} may be:
\begin{example}
  target
  limit
\end{example}
A \vn{target} \vn{merit_type} for a variable is the same as for
datum. In this case \vn{model} is just the value of the variable.
A \vn{limit} \vn{merit_type} has the form
\Begineq
  \delta V = 
    \begin{cases}
    \mbox{model} - \mbox{high\_lim}  & \mbox{model} > \mbox{high\_lim} \\
    \mbox{model} - \mbox{low\_lim}   & \mbox{model} < \mbox{low\_lim} \\
    0                                & \mbox{Otherwise}
    \end{cases}
\Endeq
The default \vn{merit_type} for a variable is \vn{limit}.

Note: when doing lattice design \vn{opt_with_ref} and
\vn{opt_with_base} are both set to \vn{False} and the \vn{target} and
\vn{limit} values are identified with \vn{Meas}.

When optimizing a storage ring, If the ring is unstable so that the
twiss parameters, closed orbit, etc. cannot be computed, the
contribution to the merit function from the corresponding datums is
set to zero. This tends to lower the merit function and in this case
an optimizer will never leave the unstable region. To avoid this,
an \vn{unstable_ring} constraint (\sref{s:data.types}) must be set.

To see a list of constraints when running \tao use the \vn{show
constraints} command (\sref{s:show}). To see how a particular variable
or datum is doing use the \vn{show data} or \vn{show variable}
commands.  See \sref{s:datum.opt} for details on how datums are
chosen to be included in an optimization.

%------------------------------------------------------------------------
\section{Variable Limits and Optimization}
\label{s:limit}

High (\vn{high_lim}) and low (\vn{low_lim}) limiting values can be set
for any variable (\sref{s:init.var}). If not explicitly set,
\vn{high_lim} defaults to $10^30$ and \vn{low_lim} defaults to
$-10^30$. When running the optimizer, if the (model) value of a
variable is outside of the range set by the limits, the value will be
set to the value of the appropriate limit and the variable's
\vn{good_user} parameter (\sref{c:var}) is set to False so that no
further variation by the optimizer is done.

If the parameter \vn{global%var_limits_on} (\sref{s:globals}) is set
to \vn{False}, limit settings are ignored. 

By default, any variable value outside of the limit range will
reset. Even those variables that are not varied by the optimizer. If
this behavior is not desired, the parameter
\vn{global%only_limit_opt_vars} may be set to \vn{True}.  If this is
done, only variables that the optimizer is allowed to vary are
restricted.

The \vn{global%optimizer_var_limit_warn} parameter controls
whether a warning is printed when a variable value goes past a limit.
The default is \vn{True}.

%------------------------------------------------------------------------
\section{Optimizers in Tao}
\label{s:tao.opti}
\index{optimization!optimizer}

The algorithm used to vary the \vn{model} variables to minimize \vn{M} is called an
\vn{optimizer}. In \vn{command line mode} the \vn{run} command is used to invoke an
\vn{optimizer}. In \vn{single mode} the \vn{g} key starts an optimizer. In both modes the
period key (\vn{``.''}) stops the optimization (however, the
\vn{global%optimizer_allow_user_abort} parameter (\sref{s:globals}) can be set to False to
prevent this). Running an optimizer is also called ``fitting'' since one is trying to get
the \vn{model} data to be equal to the \vn{measured} data. With orbits this is also called
``flattening'' since one generally wants to end up with an orbit that is on--axis.

The optimizer that is used can be defined when using the \vn{run} command but
the default optimizer can be set in the \tao input file by setting the
\vn{global%optimizer} component (\sref{s:globals}).

When the optimizer is run in \tao, the optimizer, after it initializes
itself, takes a number of \vn{cycles}. Each cycle consists of changing
the values of the variables the optimizer is allowed to change. The
number of steps that the optimizer will take is determined by the
parameter \vn{global%n_opti_cycles} (\sref{s:globals}). When the
optimizer initializes itself and goes through
\vn{global%n_opti_cycles}, it is said to have gone through one
\vn{loop}. After going through through \vn{global%n_opti_loops} loops,
the optimizer will automatically stop.  To immediately stop the
optimizer the period key \vn{``.''} may be pressed. Note: In
\vn{single_mode} (\sref{c:single}), \vn{n_opti_loops} is ignored and
the optimizer will loop forever.

There are currently three optimizers that can be used: 
  \begin{description}
  \index{lm optimizer}
  \item{\vn{lm}} \Newline
\vn{lm} is an optimizer based upon the Levenburg-Marquardt algorithm
as implemented in \vn{Numerical Recipes}\cite{b:nr}. This algorithm
looks at the local derivative matrix of \vn{dData/dVariable} and takes
steps in variable space accordingly. The derivative matrix is
calculated beforehand by varying all the variables by an amount set by
the variable's \vn{step} component (\sref{s:init.var}). The \vn{step}
size should be chosen large enough so that round-off errors will not
make computation of the derivatives inaccurate but the step size
should not be so large that the derivatives are effected by
nonlinearities. By default, the derivative matrix will be recalculated
each \vn{loop} but this can be changed by setting the
\vn{global%derivative_recalc} global parameter (\sref{s:globals}). The
reason to not recalculate the derivative matrix is one of time.
However, if the calculated derivative matrix is not accurate (that is,
if the variables have changed enough from the last time the matrix was
calculated and the nonlinearities in the lattice are large enough),
the \vn{lm} optimizer will not work very well.  In any case, this
method will only find local minimum.
  \index{lmdif optimizer}
  \item{\vn{lmdif}} \Newline
The \vn{lmdif} optimizer is like the \vn{lm} optimizer except that it
builds up the information it needs on the derivative matrix by
initially taking small steps over the first \vn{n} cycles where \vn{n}
is the number of variables. The advantage of this is that you do not
have to set a \vn{step} size for the variables. The disadvantage is
that for \vn{lmdif} to be useful, the number of \vn{cycles} must be
greater than the number of variables. Again, like \vn{lm}, this method
will only find local minimum.
  \index{de optimizer}
  \item{\vn{de}} \Newline
The \vn{de} optimizer stands for \vn{differential
evolution}\cite{b:de}. The advantage of this optimizer is that it
looks for global minimum. The disadvantage is that it is slow to find
the bottom of a local minimum. A good strategy sometimes when trying
to find a global minimum is to use \vn{de} in combination with \vn{lm}
or \vn{lmdif} one after the other. One important parameter with the
\vn{de} optimizer is the \vn{step} size. A larger step size means that
the optimizer will tend to explore larger areas of variable space but
the trade off is that this will make it harder to find minimum in the
locally. One good strategy is to vary the \vn{step} size to see what
is effective. Remember, the optimal step size will be different for
different problems and for different starting points. The \vn{step}
size that is appropriate of the \vn{de} optimizer will, in general, be
different from the \vn{step} size for the \vn{lm} optimizer. For this
reason, and to facilitate changing the step size, the actual step size
used by the \vn{de} optimizer is the step size given by a variable's
\vn{step} component multiplied by the global variable
\vn{global%de_lm_step_ratio}. This global variable can be varied using
the \vn{set} command (\sref{s:set}). The number of trial solutions used 
in the optimization is
\begin{example}
  population = number_of_variables * global%de_var_to_population_factor
\end{example}
There are also a number of parameters that can be set that will affect
how the optimizer works. See Section~\sref{s:globals} for more
details.
  \index{svd optimizer}
  \item{\vn{svd}} \Newline
The \vn{svd} optimizer uses a singular value decomposition
calculation.  See the description of \vn{svdfit} from Numerical
Recipes\cite{b:nr} for more details. With the \vn{svd} optimizer, the
setting of the \vn{global%n_opti_cycles} parameter is ignored. One
optimization loop consists of applying svd to the derivative matrix to
locate a new set of variable values.  If the merit function decreases
with the new set, the new values are retained and the optimization
loop is finished. If the merit function increases, and if the global
variable \vn{global%svd_retreat_on_merit_increase} is True (the
default), the variables are set to the original variable settings. In
either case, an increasing merit function will stop the execution of
additional loops.

The \vn{global%svd_cutoff} variable can be used to vary the cutoff
that SVD uses to decide what eigenvalues are sigular. See the
documentation for the Numerical Recipes routine \vn{svdfit} for more
details.
  \end{description}

%------------------------------------------------------------------------
\section{Optimization Troubleshooting Tips}
\label{s:opt.trouble}
\index{optimization troubleshooting}

Optimizations can behave in strange ways. Here are some tips on how to diagnose problems.

The \vn{show optimizer} (\sref{s:show.optimizer}) command will show global parameters
associated with optimizations. This will show some of the parameters that can be 
varied to get better convergence. One quick thing to do is to increase the number of
optimization loops and/or optimization cycles:
\begin{example}
	set global n_opti_loops = ...
	set global n_opti_cycles = ...
\end{example}

One of the first things to check is the merit function, the top contributors can be seen
with the command \vn{show top10} (\sref{s:show.top10}). And individual contributions
can be viewed using the \vn{show variable} and \vn{show data} commands.

If using an optimizer that uses the derivative matrix (\vn{lm}, \vn{geodesic_lm} and 
\vn{svd} optimizers), The variable \vn{step} sizes that are used to calculate the derivative
should be checked to make sure that the \vn{step} is not too small so that roundoff is a problem
but yet not too large so that nonlinearities make the calculation inaccurate. One way to
check that the step size is adequate for a given variable is to vary the variable using
the command \vn{change var} (\sref{s:change}). This command will print out the the change
in the merit function per change in variable which can be compared to the derivatives 
as shown with the \vn{show top10 -derivative} (\sref{s:show.top10}) or the
\vn{show derivative} (\sref{s:show.derivative}) command.

%------------------------------------------------------------------------
\section{Common Base Lattice (CBL) Analysis}
\label{s:cbl}
\index{common base lattice}

Some data analysis problems involve varying variables in a both the
\vn{model} and \vn{base} lattices simultaneously. Such is the case
with Orbit Response Matrix (\vn{ORM}) analysis\cite{b:orm}. With
\vn{ORM}, the analysis starts with a set of difference orbits. A given
difference orbit is generated by varying a given steering by a known
amount and the steering varied is different for different difference
orbits. Typically, The number $N$ of difference orbits is equal to the
number of steering elements in the machine. In \tao, this will result
in the creation of $N$ universes, one for each difference
measurement. The \vn{model} lattice in a universe will correspond to
the machine with the corresponding steering set to what it was when
the data was taken. Conversely, the \vn{base} lattices in all the
universes all correspond to the common condition without any steering
variation.

In \tao, this arrangement is called \vn{Common Base Lattice}
(\vn{CBL}) analysis. To do a CBL analysis, the \vn{common_lattice}
switch must be set at initialization time (\sref{s:init.lat}).  With
\vn{CBL}, \tao will set up a \vn{``common''} universe with index 0.
The \vn{model} lattice of this common universe will be used as the
\vn{base} lattice for all universes. 

The variables (fit parameters) in a \vn{CBL} analysis can be divided
into two classes. One class consists of the parameters that were
varied to get the data of the different universes. With \vn{ORM},
these are the steering strengths. At initialization
(\sref{s:init.var}), variables must be set up that control these
parameters. A single variable will control that particular parameter in
a particular universe, that was varied to create the data for that
universe. 

The second class of variables consists of everything that is
to be varied in the common base lattice. With \vn{ORM}, this generally
will include such things as quadrupole and BPM error tilts, etc. That
is, parameters that did {\em not} change during data taking. The
\tao variables that are created for these parameters will control
parameters of the \vn{model} lattice in the common universe.

To cut down on memory usage when using \vn{CBL} (the number of data
sets, hence the number of universes, can be very large), \tao does
not, except for the common \vn{model} lattice, reserve separate memory
for each \vn{model} lattice. Rather, it reserves memory for a single
\vn{``working''} lattice and the \vn{model} lattice for a particular
universe is created by first copying the common \vn{base} lattice to
the \vn{working} lattice and then applying the variable(s) (a steering
in the case of \vn{ORM}) appropriate for that universe.  As a result,
except for the common \vn{model} lattice, it is not possible to vary a
parameter of a \vn{model} lattice unless that parameter has a \tao
variable that associated with it. The \vn{change} command
(\sref{s:change}) is thus restricted to always vary parameters in
the common \vn{model} lattice.

With \vn{CBL}, the \vn{opt_with_base} and \vn{opt_with_ref}
(\sref{s:generalized.design}) global logicals are generally set to
True. Since \vn{opt_with_base}, and \vn{opt_with_ref} do not make
sense when applied to the data in the common universe, The
contribution to the merit function from data in this universe is
always calculated as if \vn{opt_with_base} and \vn{opt_with_ref} were
set to False.

With \vn{opt_with_base} set to True, the \vn{base} value for a
datum is evaluated by looking for a corresponding datum in the common
universe and using its \vn{model} value. To simplify the bookkeeping,
it is assumed that the structure of the data arrays is identical from
universe to universe. That is, the \vn{show data} command gives
identical results independent of the default universe. 
